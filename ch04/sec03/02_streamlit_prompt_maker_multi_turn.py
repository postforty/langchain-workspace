import streamlit as st
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.messages.chat import ChatMessage
from langchain_core.prompts import load_prompt
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
import glob

import os
from dotenv import load_dotenv
load_dotenv()

gemini_api_key = os.getenv("GEMINI_API_KEY")

st.title("ğŸ¤– ë‚˜ë§Œì˜ ë­ì²´ì¸ ì±—ë´‡")
st.caption("ë­ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ì±—ë´‡ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.")

if "messages" not in st.session_state:
    st.session_state.messages = []

if "task_input" not in st.session_state:
    st.session_state.task_input = ""

if "runnable" not in st.session_state:
    st.session_state.runnable = None
    st.session_state.last_prompt = None
    st.session_state.last_task = ""

with st.sidebar:
    clear_btn = st.button("ì´ˆê¸°í™”")

    prompt_files = glob.glob("prompts_multi_turn/*.yaml")
    prompt_labels = {
        'prompts_multi_turn\\general.yaml': "ì¼ë°˜ í”„ë¡¬í”„íŠ¸", 
        'prompts_multi_turn\\prompt-maker.yaml': "í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°", 
        'prompts_multi_turn\\summary.yaml': "ìš”ì•½ í”„ë¡¬í”„íŠ¸"
    }

    selected_prompt = st.selectbox(
        "í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”", 
        prompt_files, 
        index=0,
        format_func=lambda x: prompt_labels.get(x)
    )

    task_input = st.text_input(
        "TASK ì…ë ¥",
        key="task_input",
        value=st.session_state["task_input"]
    )

# print("ì„ íƒí•œ í”„ë¡¬í”„íŠ¸:", selected_prompt)
# print("ì„ íƒí•œ í”„ë¡¬í”„íŠ¸ì˜ ë‚´ìš©:", load_prompt(selected_prompt))

def print_messages():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)

def add_message(role, message):
    st.session_state["messages"].append(
        ChatMessage(role=role, content=message))

def create_chain(prompt_filepath, task=""):
    prompt = load_prompt(prompt_filepath, encoding="utf-8")

    if task:
        prompt = prompt.partial(task=task)

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash", google_api_key=gemini_api_key
    )
    output_parsers = StrOutputParser()

    chain = prompt | llm | output_parsers

    return chain

if clear_btn:
    st.session_state["messages"] = []

if "chat_histories" not in st.session_state:
    st.session_state.chat_histories = {}

print("chat_histories:", st.session_state.chat_histories)

def get_session_history(session_id):
    if session_id not in st.session_state.chat_histories:
        st.session_state.chat_histories[session_id] = ChatMessageHistory()
    return st.session_state.chat_histories[session_id]

# í”„ë¡¬í”„íŠ¸ë‚˜ TASKê°€ ë³€ê²½ë˜ì—ˆì„ ê²½ìš°ì—ë§Œ runnableì„ ìƒˆë¡œ ìƒì„±
if (
    st.session_state.runnable is None
    or st.session_state.last_prompt != selected_prompt
    or st.session_state.last_task != task_input
):
    st.session_state.last_prompt = selected_prompt
    st.session_state.last_task = task_input
    chain = create_chain(selected_prompt, task=task_input)
    st.session_state.runnable = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="question",
        history_messages_key="chat_history",
    )

print_messages()

if prompt := st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”..."):
    st.chat_message("user").markdown(prompt)
    add_message("user", prompt)

    response = st.session_state.runnable.invoke(
        {
            "question": prompt
        },
        config={"configurable": {"session_id": "any"}}
    )

    st.chat_message("assistant").markdown(response)

    add_message("assistant", response)

print(st.session_state.messages)
