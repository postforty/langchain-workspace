import streamlit as st
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage # * ìˆ˜ì •
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  # * ì¶”ê°€
from langchain_core.output_parsers import StrOutputParser  # * ì¶”ê°€
from langchain.chat_models import init_chat_model

from dotenv import load_dotenv  # * ì¶”ê°€
load_dotenv()  # * ì¶”ê°€

st.title("ğŸ¤– ë‚˜ë§Œì˜ ì±—ë´‡ ë§Œë“¤ê¸°")

st.caption("ë­ì²´ì¸ì„ ì‚¬ìš©í•œ ì±—ë´‡")

if "messages" not in st.session_state:
    st.session_state.messages = []

def print_messages():  # ëª¨ë“  ë©”ì‹œì§€ ì¶œë ¥
    for lang_message in st.session_state["messages"]:
        # LangChain ë©”ì‹œì§€ ê°ì²´ì˜ .type ì†ì„±(human, ai ë“±)ì„ Streamlit ì—­í• (user, assistant)ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.
        st_role = "user" if lang_message.type == "human" else "assistant"
        st.chat_message(st_role).write(lang_message.content)


def add_message(role, message):  # * ë©”ì‹œì§€ ì €ì¥
    # ì—­í•  ë¬¸ìì—´ì— ë”°ë¼ ì ì ˆí•œ LangChain ë©”ì‹œì§€ ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    if role == "user":
        msg_obj = HumanMessage(content=message)
    elif role == "assistant":
        msg_obj = AIMessage(content=message)
    else:
        # ì˜ˆìƒì¹˜ ëª»í•œ ì—­í• ì€ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        return
        
    st.session_state["messages"].append(msg_obj)

def create_chain():  # * ì²´ì¸
    prompt = ChatPromptTemplate.from_messages(
        [
            SystemMessage("ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
            # * ìˆ˜ì •: ëŒ€í™” ê¸°ë¡(st.session_state.messages) ì „ì²´ë¥¼ ì—¬ê¸°ì— ì‚½ì…í•©ë‹ˆë‹¤.
            MessagesPlaceholder(variable_name="messages"), # ëŒ€í™”ì˜ ì—°ì†ì„± ìœ ì§€
            HumanMessage("#Question:\n{question}"),
        ]
    )

    llm = init_chat_model("google_genai:gemini-2.5-flash-lite")

    output_parsers = StrOutputParser()

    chain = prompt | llm | output_parsers

    return chain

print_messages()

if prompt := st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”..."):
    st.chat_message("user").markdown(prompt)

    chain = create_chain()

    # 3. ëª¨ë¸ í˜¸ì¶œ: MessagesPlaceholderê°€ ë°›ëŠ” í‚¤ì¸ "messages"ì— ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ ë„˜ê¹ë‹ˆë‹¤.
    #    (ì´ ë¦¬ìŠ¤íŠ¸ì—ëŠ” ë°©ê¸ˆ add_messageë¡œ ì¶”ê°€ëœ ìƒˆë¡œìš´ HumanMessageë„ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.)
    response = chain.invoke(
        {
            "question": prompt, 
            "messages": st.session_state.messages
        }
    )

    add_message("user", prompt)
    add_message("assistant", response)

    st.chat_message("assistant").markdown(response)


