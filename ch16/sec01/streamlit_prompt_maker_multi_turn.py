from langchain_core.runnables.history import RunnableWithMessageHistory
import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, load_prompt
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
# from langchain import hub
import glob

import os
from dotenv import load_dotenv
load_dotenv()

gemini_api_key = os.getenv("GEMINI_API_KEY")

st.title("ë‚˜ë§Œì˜ LangChain ì±—ë´‡ ğŸ¤–")

if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "task_input" not in st.session_state:
    st.session_state["task_input"] = ""

if "runnable" not in st.session_state:
    st.session_state.runnable = None
    st.session_state.last_prompt = None
    st.session_state.last_task = ""


def add_message(role, message):
    st.session_state["messages"].append(
        ChatMessage(role=role, content=message))


def clear_task():
    """
    ì½œë°± í•¨ìˆ˜: ë²„íŠ¼ í´ë¦­ ì‹œ st.session_stateë¥¼ ì´ˆê¸°í™”í•œë‹¤.
    """
    st.session_state["messages"] = []
    st.session_state["task_input"] = ""
    if "any" in chat_histories:
        del chat_histories["any"]


with st.sidebar:
    st.button("ëŒ€í™” ì´ˆê¸°í™”", on_click=clear_task)

    prompt_files = glob.glob("prompts_multi_turn/*.yaml")

    # ì…€ë ‰ìŠ¤ ë°•ìŠ¤ì— íŒŒì¼ ê²½ë¡œ í‘œì‹œ
    # print("prompt_files:", prompt_files)
    # selected_prompt = st.selectbox(
    #     "í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”", prompt_files, index=0
    # )

    # íŒŒì¼ ê²½ë¡œë¥¼ ì‚¬ìš©ì ì¹œí™”ì ì¸ ë ˆì´ë¸”ë¡œ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
    prompt_labels = {
        "prompts_multi_turn\\general-chat-history.yaml": "ì¼ë°˜ í”„ë¡¬í”„íŠ¸",
        "prompts_multi_turn\\prompt-maker.yaml": "í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°",
        "prompts_multi_turn\\summary.yaml": "ìš”ì•½ í”„ë¡¬í”„íŠ¸",
    }

    selected_prompt = st.selectbox(
        "í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”",
        prompt_files,
        index=0,
# get() ë©”ì„œë“œëŠ” ì§€ì •ëœ í‚¤ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ê°€ì ¸ì˜¤ëŠ” ì—­í• , ë§Œì•½ í•´ë‹¹ í‚¤ê°€ ë”•ì…”ë„ˆë¦¬ì— ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°, ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚¤ëŠ” ëŒ€ì‹  ê¸°ë³¸ê°’(ë‘ë²ˆì§¸ ì¸ìˆ˜)ì„ ë°˜í™˜
        format_func=lambda x: prompt_labels.get(x, x),  # íŒŒì¼ ê²½ë¡œë¥¼ ë ˆì´ë¸”ë¡œ ë³€í™˜
    )

    task_input = st.text_input(
        "TASK ì…ë ¥", key="task_input", value=st.session_state["task_input"])

print("ì„ íƒëœ í”„ë¡¬í”„íŠ¸:", selected_prompt)
print("ì„ íƒëœ í”„ë¡¬í”„íŠ¸ ë‚´ìš©:", load_prompt(selected_prompt, encoding="utf-8"))


for msg in st.session_state.messages:
    st.chat_message(msg.role).write(msg.content)


def create_chain(prompt_filepath, task=""):
    # prompt = ChatPromptTemplate.from_messages(
    #     [
    #         ("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
    #         ("user", "#Question:\n{question}"),
    #     ]
    # )

    prompt = load_prompt(prompt_filepath, encoding="utf-8")

    if task:
        prompt = prompt.partial(task=task)

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0,
        google_api_key=gemini_api_key)

    output_parsers = StrOutputParser()

    chain = prompt | llm | output_parsers

    return chain


# ì„¸ì…˜ë³„ ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬
chat_histories = {}

# ì„¸ì…˜ IDì— ë”°ë¼ ëŒ€í™” ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜


def get_session_history(session_id: str):
    if session_id not in chat_histories:
        chat_histories[session_id] = ChatMessageHistory()
    return chat_histories[session_id]


# í”„ë¡¬í”„íŠ¸ë‚˜ TASKê°€ ë³€ê²½ë˜ì—ˆì„ ê²½ìš°ì—ë§Œ runnableì„ ìƒˆë¡œ ìƒì„±
if (
    st.session_state.runnable is None
    or st.session_state.last_prompt != selected_prompt
    or st.session_state.last_task != task_input
):
    st.session_state.last_prompt = selected_prompt
    st.session_state.last_task = task_input
    chain = create_chain(selected_prompt, task=task_input)
    st.session_state.runnable = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="question",
        history_messages_key="chat_history",
    )


user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

if user_input:
    st.chat_message("user").write(user_input)
    # add_message("user", user_input)

    response = st.session_state.runnable.stream(
        {"question": user_input}, config={"configurable": {"session_id": "any"}}
    )

    with st.chat_message("assistant"):
        container = st.empty()

        ai_answer = ""

        for token in response:
            ai_answer += token
            container.markdown(ai_answer)

    add_message("user", user_input)
    add_message("assistant", ai_answer)

print("st.session_state.messages:", st.session_state.messages)


# [í…ŒìŠ¤íŠ¸]
# í”„ë¡¬í”„íŠ¸ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”: prompt-maker.yaml ì„ íƒ
# TASK ì…ë ¥: ë¸”ëŸ¬ê·¸ ê¸€ ì‘ì„±
# ì…ë ¥ í”„ë¡¬í”„íŠ¸: ë­ì²´ì¸ì´ë¼ëŠ” ì£¼ì œë¡œ ê¸€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”
