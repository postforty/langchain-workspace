import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import load_prompt

import os
from dotenv import load_dotenv
load_dotenv()

st.title("ğŸ“„PDF ê¸°ë°˜ QA")

if not os.path.exists(".cache"): # * í´ë” ì•ì— .ì„ ë¶™ì´ë©´ ìˆ¨ê¹€ ì²˜ë¦¬í•¨(Linux, macOS)ì„ ì˜ë¯¸
    os.mkdir(".cache")
    # * Windowsì—ì„œ .cache í´ë”ë¥¼ ìˆ¨ê¹€ ì²˜ë¦¬
    if os.name == 'nt':
        os.system('attrib +h .cache')

if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")

if "messages" not in st.session_state:
    st.session_state["messages"] = []

@st.cache_resource(show_spinner="ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

def create_chain(prompt_filepath):
    prompt = load_prompt(prompt_filepath, encoding="utf-8")

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0)

    output_parsers = StrOutputParser()

    chain = prompt | llm | output_parsers

    return chain


def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)

def add_message(role, message):
    st.session_state["messages"].append(
        ChatMessage(role=role, content=message))

# * ëŒ€í™” ì´ˆê¸°í™”
def clear_task():
    st.session_state["messages"] = []

with st.sidebar:
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”", on_click=clear_task) # * on_click ì¸ìë¡œ clear_task í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œ

    uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])

    selected_prompt = "prompts/pdf-rag.yaml"

# print("selected_prompt:", selected_prompt)

if uploaded_file:
    embed_file(uploaded_file)

user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

print_messages()

if user_input:
    st.chat_message("user").write(user_input)

    chain = create_chain(selected_prompt)
    response = chain.stream({"question": user_input, "context": ""})

    with st.chat_message("assistant"):
        container = st.empty()

        ai_answer = ""

        for token in response:
            ai_answer += token
            container.markdown(ai_answer)

    add_message("user", user_input)
    add_message("assistant", ai_answer)

print("st.session_state.messages:", st.session_state.messages)

#  ë³¸ ì—°êµ¬ì—ì„œ Private LLM êµ¬ì¶•ì„ ìœ„í•´ ìˆ˜ì§‘í•œ ë¬¸ì„œì˜ ì´ í˜ì´ì§€ ìˆ˜ì™€ ë¬¸ì„œ ìœ í˜•ë³„ ë¹„ìœ¨ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?
