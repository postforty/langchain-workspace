import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import load_prompt
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_core.runnables import RunnablePassthrough

import shutil  # * íŒŒì¼ ë° ë””ë ‰í† ë¦¬ ì‘ì—…ìš©
import uuid  # * ê³ ìœ  í‚¤ ìƒì„±ì„ ìœ„í•œ uuid ëª¨ë“ˆ ì„í¬íŠ¸

import os
from dotenv import load_dotenv
load_dotenv()

st.title("ğŸ“„PDF ê¸°ë°˜ QA")
st.caption("gemini-embedding-001 + Gemini-2.5-FLASH") # * ìº¡ì…˜ ì¶”ê°€

if not os.path.exists(".cache"): # * í´ë” ì•ì— .ì„ ë¶™ì´ë©´ ìˆ¨ê¹€ ì²˜ë¦¬í•¨(Linux, macOS)ì„ ì˜ë¯¸
    os.mkdir(".cache")
    # * Windowsì—ì„œ .cache í´ë”ë¥¼ ìˆ¨ê¹€ ì²˜ë¦¬
    if os.name == 'nt':
        os.system('attrib +h .cache')

if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")

if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "chain" not in st.session_state:
    st.session_state["chain"] = None

# * í—¬í¼ í•¨ìˆ˜ ì •ì˜
# ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë˜ëŠ” ë¡œë“œ
def _get_or_create_vectorstore(file_name, splitted_documents=None):
    # ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„
    # gemini-embedding-001 ëª¨ë¸ì€ QUOTA ì˜¤ë¥˜ ë°œìƒí•  ìˆ˜ ìˆìŒ
    # ì°¸ê³ ) https://ai.google.dev/gemini-api/docs/rate-limits?hl=ko
    embedding_model = GoogleGenerativeAIEmbeddings(
        model="gemini-embedding-001",
        transport='rest' # Streamlitì˜ ë™ê¸°ì ì¸ í™˜ê²½ê³¼ í˜¸í™˜ë˜ë„ë¡ ì„¤ì •(ê¸°ë³¸ê°’ì€ ë¹„ë™ê¸°)
    )

    embedding_path = f".cache/embeddings/{file_name}"
    vectorstore = None
    if splitted_documents is not None:  # If new documents are provided, always create
        print(f"FAISS ì¸ë±ìŠ¤ {embedding_path}ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        if os.path.exists(embedding_path):  # Remove old one if exists
            shutil.rmtree(embedding_path)
        vectorstore = FAISS.from_documents(splitted_documents, embedding_model)
        vectorstore.save_local(embedding_path)
        print(f"FAISS ì¸ë±ìŠ¤ë¥¼ {embedding_path}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    elif os.path.exists(embedding_path):  # Otherwise, try to load existing
        print(f"FAISS ì¸ë±ìŠ¤ {embedding_path}ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
        vectorstore = FAISS.load_local(
            embedding_path,
            embedding_model,
            allow_dangerous_deserialization=True,
        )

    return vectorstore

@st.cache_resource(show_spinner="ì—…ë¡œë“œí•œ íŒŒì¼ ì²˜ë¦¬ ì¤‘...", ttl=3600) # * ttl=3600ìœ¼ë¡œ 1ì‹œê°„ ë™ì•ˆ ìºì‹œëœ ê²°ê³¼ ì‚¬ìš© ê°€ëŠ¥, ë¶ˆí•„ìš”í•œ ì—°ì‚° ì¤„ì„(í•„ìˆ˜ ì•„ë‹˜)
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    # * ë¬¸ì„œ ë¡œë“œ
    # loader = PDFPlumberLoader(file_path)
    loader = PyMuPDFLoader(file_path)
    documents = loader.load()

    # * ë¬¸ì„œ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
    )
    splitted_documents = text_splitter.split_documents(documents)

    # print("splitted_documents:", splitted_documents)

    # * ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë˜ëŠ” ë¡œë“œ
    vectorstore = _get_or_create_vectorstore(file.name, splitted_documents)

    # * ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
    retriever = vectorstore.as_retriever()
    return retriever

def create_chain(retriever, prompt_filepath):
    prompt = load_prompt(prompt_filepath, encoding="utf-8")

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0)

    output_parsers = StrOutputParser()

    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | output_parsers
    )

    return chain


def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)

def add_message(role, message):
    st.session_state["messages"].append(
        ChatMessage(role=role, content=message))

# * ëŒ€í™” ì´ˆê¸°í™”
def clear_task():
    st.session_state["messages"] = []

with st.sidebar:
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”", on_click=clear_task) # * on_click ì¸ìë¡œ clear_task í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œ

    uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])

    selected_prompt = "prompts/pdf-rag.yaml"

# print("selected_prompt:", selected_prompt)

if uploaded_file:
    # * ë²¡í„° ì €ì¥ì†Œ, ì²´ì¸ ìƒì„±
    retriever = embed_file(uploaded_file)
    chain = create_chain(retriever, selected_prompt)
    st.session_state["chain"] = chain

user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

warning_msg = st.empty() # * íŒŒì¼ ì—…ë¡œë“œ ê²½ê³  ë©”ì‹œì§€

print_messages()

if user_input:
    if st.session_state["chain"] is not None:
        st.chat_message("user").write(user_input)
        response = st.session_state["chain"].stream(user_input) # * RunnablePassthrough()ì— ë¬¸ìì—´ ì „ë‹¬

        with st.chat_message("assistant"):
            container = st.empty()

            ai_answer = ""

            for token in response:
                ai_answer += token
                container.markdown(ai_answer)

        add_message("user", user_input)
        add_message("assistant", ai_answer)
    else:
        warning_msg.error("íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")

print("st.session_state.messages:", st.session_state.messages)

#  ë³¸ ì—°êµ¬ì—ì„œ Private LLM êµ¬ì¶•ì„ ìœ„í•´ ìˆ˜ì§‘í•œ ë¬¸ì„œì˜ ì´ í˜ì´ì§€ ìˆ˜ì™€ ë¬¸ì„œ ìœ í˜•ë³„ ë¹„ìœ¨ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?
